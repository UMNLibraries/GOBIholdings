{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This work is copyright (c) the Regents of the University of Minnesota, 2017.  \n",
    "# It was created and last updated by Kelly Thompson, 2018-06-27.\n",
    "\n",
    "# We anticipate adding an open license to this software, pending administrative approval, but in the meantime,\n",
    "# please just contact us to ask.  Thanks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib3\n",
    "from lxml import etree\n",
    "import pandas as pd\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "today = str(date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the Alma Analytics API to retrieve the GOBI holdings reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_root = 'https://api-eu.hosted.exlibrisgroup.com/almaws/v1/analytics/reports'\n",
    "key = 'REDACTED'\n",
    "limit = '1000'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "paths = ['/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 1990-1999 - ids not excluded',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 2000-2009 - ids not excluded',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 2010-2019 - ids not excluded',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/Duluth GOBI electronic holdings 1990-2019 - ids not excluded',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/Duluth GOBI physical holdings 1990-2019 pub dates - full',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 1990-1994 pub dates - full',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 1995-1999 pub dates - full',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2000-2004 pub dates - full',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2005-2009 pub dates - full',\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2010-2019 pub dates - full'\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = ['TC_GOBI_electronic_holdings_1990-1999_ids_not_excluded',\n",
    "           'TC_GOBI_electronic_holdings_2000-2009_ids_not_excluded',\n",
    "           'TC_GOBI_electronic_holdings_2010-2019_ids_not_excluded',\n",
    "           'Duluth_GOBI_electronic_holdings_1990-2019_full',\n",
    "           'Duluth_GOBI_physical_holdings_1990-2019_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_1990-1994_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_1995-1999_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_2000-2004_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_2005-2009_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_2010-2019_pub_dates_full'\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_report (path, url_root, key, limit):\n",
    "    params2 = {'apikey': key, 'path': path, 'limit': limit}\n",
    "    r = requests.get(url_root, params = params2)\n",
    "    print(r.url)\n",
    "    print(r.status_code)\n",
    "    if r.status_code != 200:\n",
    "        print(r.text)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revised\n",
    "\n",
    "def get_text(r, index, url_root, key, limit):\n",
    "    \n",
    "    \n",
    "    print(\"Chunk 1\")\n",
    "    \n",
    "    root = etree.fromstring(r.content)\n",
    "    #print(etree.dump(root))\n",
    "    \n",
    "    resultset = root.findall(\"./QueryResult/ResultXml/*/*\")\n",
    "    print(\"number of results \" + str(len(resultset)))\n",
    "    \n",
    "    a = etree.Element('response')\n",
    "    #etree.dump(a)\n",
    "    resultset.pop(0)\n",
    "    for each in resultset:\n",
    "        a.append(deepcopy(each))\n",
    "    b = a.getchildren()\n",
    "    print(len(b))\n",
    "    \n",
    "    print(\"Chunk 2\")\n",
    "    \n",
    "    f = root.find(\"./QueryResult/IsFinished\")\n",
    "    if f.text == \"false\":\n",
    "        token = root.find(\"./QueryResult/ResumptionToken\")\n",
    "        \n",
    "    while (f.text == \"false\"):\n",
    "\n",
    "        if token.text != None:\n",
    "            params3 = {'apikey': key, 'token': token.text}\n",
    "            r2 = requests.get(url_root, params = params3 )\n",
    "            print(r2.url)\n",
    "\n",
    "            root2 = etree.fromstring(r2.content)\n",
    "            resultset2 = root2.findall(\"./QueryResult/ResultXml/*/*\")\n",
    "            print(len(resultset2))\n",
    "\n",
    "            for each in resultset2:\n",
    "                a.append(deepcopy(each))\n",
    "            c = a.getchildren()\n",
    "            print(len(c))\n",
    "\n",
    "            f = root2.find(\"./QueryResult/IsFinished\")\n",
    "            \n",
    "            if f is None:\n",
    "                \n",
    "                print(token.text)\n",
    "                \n",
    "    else:\n",
    "        print(\"done looping resumption tokens\")\n",
    "            \n",
    "    print(\"Chunk 4\")\n",
    "\n",
    "    all_records = []\n",
    "    rows = a.getchildren()\n",
    "    print(len(rows))\n",
    "    for each in rows:\n",
    "        record = {}\n",
    "        gcs = each.getchildren()\n",
    "        for column in gcs:\n",
    "            record[column.tag] = column.text\n",
    "        all_records.append(record)\n",
    "    data = pd.DataFrame(all_records)\n",
    "\n",
    "    print(\"Chunk 5\")\n",
    "\n",
    "    output_file = outputs[index] + '-' + today + '.csv'\n",
    "    print(output_file)\n",
    "    data.to_csv(output_file)\n",
    "    print(\"Success!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FOR TESTING ONLY\n",
    "paths = [#'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 1990-1999 - ids not excluded',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 2000-2009 - ids not excluded',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI electronic holdings 2010-2019 - ids not excluded',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/Duluth GOBI electronic holdings 1990-2019 - ids not excluded',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/Duluth GOBI physical holdings 1990-2019 pub dates - full',\n",
    "        #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 1990-1994 pub dates - full',\n",
    "        #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 1995-1999 pub dates - full',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 1990-1999 pub dates - full',\n",
    "         #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/drafts/TC GOBI physical holdings 2000-09 pub dates - full',\n",
    "        #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2000-2004 pub dates - full',\n",
    "        #'/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2005-2009 pub dates - full'\n",
    "         '/shared/University of Minnesota/Twin Cities/Kelly Thompson/GOBI holdings/TC GOBI physical holdings 2010-2019 pub dates - full'\n",
    "        ]\n",
    "\n",
    "outputs = [#'TC_GOBI_electronic_holdings_1990-1999_ids_not_excluded',\n",
    "           #'TC_GOBI_electronic_holdings_2000-2009_ids_not_excluded',\n",
    "           #'TC_GOBI_electronic_holdings_2010-2019_ids_not_excluded',\n",
    "           #'Duluth_GOBI_electronic_holdings_1990-2019_full',\n",
    "           #'Duluth_GOBI_physical_holdings_1990-2019_pub_dates_full',\n",
    "           #'TC_GOBI_physical_holdings_1990-1999_pub_dates_full',\n",
    "            #'TC_GOBI_physical_holdings_1990-1994_pub_dates_full',\n",
    "            #'TC_GOBI_physical_holdings_1995-1999_pub_dates_full'\n",
    "           #'TC_GOBI_physical_holdings_2000-09_pub_dates_full',\n",
    "            #'TC_GOBI_physical_holdings_2000-2004_pub_dates_full',\n",
    "            #'TC_GOBI_physical_holdings_2005-2009_pub_dates_full',\n",
    "           'TC_GOBI_physical_holdings_2010-2019_pub_dates_full',\n",
    "           \n",
    "          ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_of_tries = []\n",
    "index = 0\n",
    "for path in paths:\n",
    "    i = 1\n",
    "    print(\"try \" + str(i))\n",
    "    r = fetch_report(path, url_root, key, limit)\n",
    "    while r.status_code != 200:\n",
    "        i += 1\n",
    "        print(\"try \" + str(i))\n",
    "        r = fetch_report(path, url_root, key, limit)\n",
    "    print(\"DONE\")\n",
    "    num_of_tries.append(i)\n",
    "    print(num_of_tries)\n",
    "    \n",
    "    %timeit -n1 -r1 -c get_text(r, index, url_root, key, limit)\n",
    "    \n",
    "    index += 1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Now we should have all of our reports, which we can process to keep only the data we need, in the format required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "folder = \"2018-06-26\"\n",
    "# get lists of files\n",
    "tce_files = [f for f in os.listdir(folder) if re.match('.*TC.*.elec.*.csv', f)]\n",
    "tce_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_files = [f for f in os.listdir(folder) if re.match('.*TC.*.phys.*.csv', f)]\n",
    "tcp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dule_files = [f for f in os.listdir(folder) if re.match('Dul.*.elec.*.csv', f)]\n",
    "dule_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dulp_files = [f for f in os.listdir(folder) if re.match('Dul.*.phys.*.csv', f)]\n",
    "dulp_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all TC electronic reports from Analytics\n",
    "tce = pd.DataFrame()\n",
    "print(tce)\n",
    "for f in tce_files:\n",
    "    tce_new = pd.read_csv(folder + \"\\\\\" + f, sep = \",\", encoding = \"utf-8\", dtype = str,\n",
    "                          usecols=['{urn:schemas-microsoft-com:xml-analysis:rowset}Column11',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column6'])\n",
    "    tce = pd.concat([tce, tce_new], ignore_index=True)\n",
    "tce.rename(columns={\"{urn:schemas-microsoft-com:xml-analysis:rowset}Column11\": \"ISBN RegEx\",\n",
    "                    \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column6\": \"ElectronicCollection\"}, inplace=True)\n",
    "tce['Library Code 10 char'] = 'TCEBOOK'\n",
    "tce['YBP sub-account'] = str('195099')\n",
    "tce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tc_ecolls_to_exclude = pd.read_csv('IDs to exclude\\elec colls to exclude\\TC electronic colls to exclude.txt',\n",
    "                                   sep='\\t', dtype=str)\n",
    "tc_ecolls_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tce2 = tce[~tce['ElectronicCollection'].isin(tc_ecolls_to_exclude['ecoll'])]\n",
    "tce2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all TC physical reports from Analytics\n",
    "tcp = pd.DataFrame()\n",
    "print(tcp)\n",
    "for f in tcp_files:\n",
    "    tcp_new = pd.read_csv(folder + \"\\\\\" + f, sep = \",\", encoding = \"utf-8\", dtype = str,\n",
    "                          usecols=['{urn:schemas-microsoft-com:xml-analysis:rowset}Column7',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column8',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column6'])\n",
    "    tcp = pd.concat([tcp, tcp_new], ignore_index=True)\n",
    "tcp.rename(columns={\"{urn:schemas-microsoft-com:xml-analysis:rowset}Column7\" : \"ISBN RegEx\", \n",
    "                    \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column8\" : \"Library Code 10 char\", \n",
    "                    \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column6\" : \"YBP sub-account\"}, \n",
    "           inplace=True)\n",
    "tcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tce2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tce2.drop(columns=['ElectronicCollection'],inplace=True)\n",
    "tce2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate tc physcial and electronic dataframes into one\n",
    "tc = pd.concat([tce2, tcp], ignore_index=True)\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all Duluth electronic reports from Analytics\n",
    "dule = pd.DataFrame()\n",
    "print(dule)\n",
    "for f in dule_files:\n",
    "    dule_new = pd.read_csv(folder + \"\\\\\" + f, sep = \",\", encoding = \"utf-8\", dtype = str,\n",
    "                          usecols=['{urn:schemas-microsoft-com:xml-analysis:rowset}Column11',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column6'])\n",
    "    dule = pd.concat([dule, dule_new], ignore_index=True)\n",
    "dule.rename(columns={\"{urn:schemas-microsoft-com:xml-analysis:rowset}Column11\": \"ISBN RegEx\",\n",
    "                     \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column6\": \"ElectronicCollection\"}, \n",
    "            inplace=True)\n",
    "dule['Library Code 10 char'] = 'DULEBOOK'\n",
    "dule['YBP sub-account'] = str('583999')\n",
    "dule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dul_ecolls_to_exclude = pd.read_csv('IDs to exclude\\elec colls to exclude\\Duluth electronic colls to exclude.txt',\n",
    "                                   sep='\\t', dtype=str)\n",
    "dul_ecolls_to_exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dule2 = dule[~dule['ElectronicCollection'].isin(dul_ecolls_to_exclude['ecoll'])]\n",
    "dule2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in all TC physical reports from Analytics\n",
    "dulp = pd.DataFrame()\n",
    "print(dulp)\n",
    "for f in dulp_files:\n",
    "    dulp_new = pd.read_csv(folder + \"\\\\\" + f, sep = \",\", encoding = \"utf-8\", dtype = str,\n",
    "                          usecols=['{urn:schemas-microsoft-com:xml-analysis:rowset}Column7',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column8',\n",
    "                                   '{urn:schemas-microsoft-com:xml-analysis:rowset}Column6'])\n",
    "    dulp = pd.concat([dulp, dulp_new], ignore_index=True)\n",
    "dulp.rename(columns={\"{urn:schemas-microsoft-com:xml-analysis:rowset}Column7\" : \"ISBN RegEx\", \n",
    "                    \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column8\" : \"Library Code 10 char\", \n",
    "                    \"{urn:schemas-microsoft-com:xml-analysis:rowset}Column6\" : \"YBP sub-account\"}, \n",
    "           inplace=True)\n",
    "dulp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dule2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dule2.drop(columns=['ElectronicCollection'],inplace=True)\n",
    "dule2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dulp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concatenate tc physcial and electronic dataframes into one\n",
    "dul = pd.concat([dule2, dulp], ignore_index=True)\n",
    "dul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# remove invalid TC ISBNs from set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_isbns = tc['ISBN RegEx']\n",
    "print(tc_isbns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tc.drop_duplicates(inplace=True)\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_isbns = tc['ISBN RegEx']\n",
    "print(tc_isbns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tc_isbns = pd.read_table(\"C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC and Law invalid ISBNs.txt\")\n",
    "bad_tc_isbns = bad_tc_isbns['ISBN']\n",
    "print(bad_tc_isbns.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc1 = tc[~tc['ISBN RegEx'].isin(bad_tc_isbns)]\n",
    "tc1['ISBN RegEx'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_tc_e_isbns = pd.read_table(\"C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/TC electronic ISBNs to exclude.txt\")\n",
    "bad_tc_e_isbns = bad_tc_e_isbns['ISBN']\n",
    "bad_tc_e_isbns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc2 = tc1[~tc1['ISBN RegEx'].isin(bad_tc_e_isbns)]\n",
    "tc2['ISBN RegEx'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_final = tc2.rename(columns={'ISBN RegEx':'ISBN'})\n",
    "tc_final.to_csv(folder + \"\\\\\" + 'TC-all.txt', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################\n",
    "# create law file and fix law account number\n",
    "\n",
    "law = tc_final\n",
    "law['YBP sub-account'].replace(\"195099\", \"590099\", inplace= True)\n",
    "law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law.to_csv(folder + \"\\\\\" + \"law-all.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# remove invalid duluth isbns\n",
    "\n",
    "dul_isbns = dul['ISBN RegEx']\n",
    "dul_isbns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dul.drop_duplicates(inplace=True)\n",
    "dul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_dul_isbns = pd.read_table(\"C:/Users/kjthomps/Documents/GOBI holdings reports/IDs to exclude/Duluth invalid ISBNs.txt\")\n",
    "bad_dul_isbns = bad_dul_isbns['ISBN']\n",
    "bad_dul_isbns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dul1 = dul[~dul['ISBN RegEx'].isin(bad_dul_isbns)]\n",
    "dul1['ISBN RegEx'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dul_final = dul1.rename(columns={'ISBN RegEx':'ISBN'})\n",
    "dul_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dul_final.to_csv(folder + \"\\\\\" + \"Duluth-all.txt\", sep=\"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the current month's inventory, we need to find the intersections with the previous inventory, to produce files of adds and deletes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_folder = folder + \"\\\\last batch\"\n",
    "# get lists of files\n",
    "old_TC_file = [f for f in os.listdir(old_folder) if re.match('.*TC.*', f)][0]\n",
    "old_TC_file = old_folder + \"\\\\\" + old_TC_file\n",
    "old_TC_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_TC_file = folder + \"\\TC-all.txt\"\n",
    "new_TC_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_adds = \"TC-adds.txt\"\n",
    "TC_deletes = \"TC-deletes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "law_adds_file = \"LAW-adds.txt\"\n",
    "law_deletes_file = \"LAW-deletes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_DUL_file = [f for f in os.listdir(old_folder) if re.match('.*Dul.*', f)][0]\n",
    "old_DUL_file = old_folder + \"\\\\\" + old_DUL_file\n",
    "old_DUL_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_DUL_file = folder + \"\\Duluth-all.txt\"\n",
    "new_DUL_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dul_adds = \"DUL-adds.txt\"\n",
    "dul_deletes = \"DUL-deletes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adds_dels (old_file, new_file):\n",
    "    \n",
    "    old_df = pd.read_table(old_file)\n",
    "    old_df = old_df.drop_duplicates()\n",
    "    \n",
    "    print(old_df.columns)\n",
    "    \n",
    "    new_df = pd.read_table(new_file)\n",
    "    new_df = new_df.drop_duplicates()\n",
    "    \n",
    "    print(new_df.columns)\n",
    "    \n",
    "    old_df = old_df.rename(index=str, columns={\"ISBN\": \"ISBN_old\", \"Library Code 10 char\": \"LibraryCode_old\", \"YBP sub-account\" : \"YBPacct_old\"})\n",
    "    \n",
    "    new_df = new_df.rename(index=str, columns={\"ISBN\": \"ISBN_new\", \"Library Code 10 char\": \"LibraryCode_new\", \"YBP sub-account\" : \"YBPacct_new\"})\n",
    "    \n",
    "    old_merged = pd.merge(left = old_df, right = new_df, how='left', left_on=['ISBN_old','LibraryCode_old'], right_on=['ISBN_new','LibraryCode_new'])\n",
    "    deleted = old_merged[pd.isnull(old_merged['LibraryCode_new'])]\n",
    "    deleted = deleted.loc[:,['ISBN_old','LibraryCode_old','YBPacct_old']]\n",
    "    \n",
    "    \n",
    "    new_merged = pd.merge(left = old_df, right = new_df, how='right', left_on=['ISBN_old','LibraryCode_old'], right_on=['ISBN_new','LibraryCode_new'])\n",
    "    added = new_merged[pd.isnull(new_merged['LibraryCode_old'])]\n",
    "    added = added.loc[:,['ISBN_new','LibraryCode_new','YBPacct_new']]\n",
    "    \n",
    "    deletes = pd.merge(left=old_df, right=new_df, how = \"left\", left_on=['ISBN_old','LibraryCode_old'], \n",
    "                       right_on=['ISBN_new','LibraryCode_new'])\n",
    "    deletes = deletes[pd.isnull(deletes['LibraryCode_new'])]\n",
    "    deletes = deletes.loc[:,['ISBN_old','LibraryCode_old','YBPacct_old']]\n",
    "    \n",
    "    adds = pd.merge(left=old_df, right=new_df, how = \"right\", left_on=['ISBN_old','LibraryCode_old'], \n",
    "                       right_on=['ISBN_new','LibraryCode_new'])\n",
    "    adds = adds[pd.isnull(adds['LibraryCode_old'])]\n",
    "    adds = adds.loc[:,['ISBN_new','LibraryCode_new','YBPacct_new']]\n",
    "    \n",
    "    adds = adds.rename(index=str, columns={\"ISBN_new\" : \"ISBN\", \"LibraryCode_new\" : \"Library Code 10 char\", \n",
    "                                                             \"YBPacct_new\" : \"YBP sub-account\"})\n",
    "    \n",
    "    deletes = deletes.rename(index=str, columns={\"ISBN_old\" : \"ISBN\", \"LibraryCode_old\" : \"Library Code 10 char\", \n",
    "                                                             \"YBPacct_old\" : \"YBP sub-account\"})\n",
    "    \n",
    "    return adds, deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds, deletes = find_adds_dels(old_TC_file, new_TC_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds.to_csv(folder + \"\\\\\" + TC_adds, sep=\"\\t\", index = False)\n",
    "deletes.to_csv(folder + \"\\\\\" + TC_deletes, sep=\"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adds[\"YBP sub-account\"].replace(195099, 590099, inplace= True)\n",
    "adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adds.to_csv(folder + \"\\\\\" + law_adds_file, sep=\"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deletes[\"YBP sub-account\"].replace(195099, 590099, inplace= True)\n",
    "deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletes.to_csv(folder + \"\\\\\" + law_deletes_file, sep=\"\\t\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds, deletes = find_adds_dels(old_DUL_file, new_DUL_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deletes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adds.to_csv(folder + \"\\\\\" + dul_adds, sep=\"\\t\", index = False)\n",
    "deletes.to_csv(folder + \"\\\\\" + dul_deletes, sep=\"\\t\", index = False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
